{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515eb7a6-90de-4788-ba96-f84e756722da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Retail Data Wrangling and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bbe8ba-70a0-422b-9189-f480c8fac718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381e8d5b-6b72-451a-85b3-8293f5beb6f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load CSV into Dataframe\n",
    "Alternatively, the LGS IT team also dumped the transactional data into a [CSV file](https://raw.githubusercontent.com/jarviscanada/jarvis_data_eng_demo/feature/data/python_data_wrangling/data/online_retail_II.csv). However, the CSV header (column names) doesn't follow the snakecase or camelcase naming convention (e.g. `Customer ID` instead of `customer_id` or `CustomerID`). As a result, you will need to use Pandas to clean up the data before doing any analytics. In addition, unlike the PSQL scheme, CSV files do not have data types associated. Therefore, you will need to cast/convert certain columns into correct data types (e.g. DateTime, numbers, etc..)\n",
    "\n",
    "**Data Preperation**\n",
    "\n",
    "- Read the `data/online_retail_II.csv` file into a DataFrame\n",
    "- Rename all columns to upper camelcase or snakecase\n",
    "- Convert/cast all columns to the appropriate data types (e.g. datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7569ba12-cdce-487c-bcdc-92732b128204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+\n|Invoice|StockCode|         Description|Quantity|        InvoiceDate|Price|Customer ID|       Country|\n+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+\n| 557112|    23243|SET OF TEA COFFEE...|       1|2011-06-16 16:31:00| 4.95|       NULL|United Kingdom|\n| 557112|    23245|SET OF 3 REGENCY ...|       4|2011-06-16 16:31:00|10.79|       NULL|United Kingdom|\n| 557112|    23251|VINTAGE RED ENAME...|       2|2011-06-16 16:31:00| 2.46|       NULL|United Kingdom|\n| 557112|    23256|CHILDRENS CUTLERY...|       1|2011-06-16 16:31:00| 8.29|       NULL|United Kingdom|\n| 557112|    23298|      SPOTTY BUNTING|       2|2011-06-16 16:31:00|10.79|       NULL|United Kingdom|\n| 557112|    23300|GARDENERS KNEELIN...|       2|2011-06-16 16:31:00| 3.29|       NULL|United Kingdom|\n| 557112|    23301|GARDENERS KNEELIN...|       5|2011-06-16 16:31:00| 3.29|       NULL|United Kingdom|\n| 557112|    23306|SET OF 36 DOILIES...|       2|2011-06-16 16:31:00| 2.92|       NULL|United Kingdom|\n| 557112|    23307|SET OF 60 PANTRY ...|       3|2011-06-16 16:31:00| 1.25|       NULL|United Kingdom|\n| 557112|    23308|SET OF 60 VINTAGE...|       2|2011-06-16 16:31:00| 1.25|       NULL|United Kingdom|\n| 557112|    23310|BUBBLEGUM RING AS...|       2|2011-06-16 16:31:00| 0.83|       NULL|United Kingdom|\n| 557112|    23322|LARGE WHITE HEART...|       1|2011-06-16 16:31:00| 5.83|       NULL|United Kingdom|\n| 557112|    23336|EGG FRYING PAN PINK |       1|2011-06-16 16:31:00| 4.13|       NULL|United Kingdom|\n| 557112|    23337|EGG FRYING PAN MINT |       1|2011-06-16 16:31:00| 4.13|       NULL|United Kingdom|\n| 557112|   35004B|SET OF 3 BLACK FL...|       1|2011-06-16 16:31:00| 5.79|       NULL|United Kingdom|\n| 557112|   35471D|SET OF 3 BIRD LIG...|       1|2011-06-16 16:31:00| 1.25|       NULL|United Kingdom|\n| 557112|   35598D|PINK/WHITE CHRIST...|       1|2011-06-16 16:31:00| 2.46|       NULL|United Kingdom|\n| 557112|   35911A|MULTICOLOUR RABBI...|       4|2011-06-16 16:31:00| 0.83|       NULL|United Kingdom|\n| 557112|    37450|CERAMIC CAKE BOWL...|       1|2011-06-16 16:31:00| 5.79|       NULL|United Kingdom|\n| 557112|    37475|SET/4 COLOURFUL M...|       1|2011-06-16 16:31:00| 8.29|       NULL|United Kingdom|\n+-------+---------+--------------------+--------+-------------------+-----+-----------+--------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "retail_df = spark.read.table(\"workspace.default.online_retail_ii\")\n",
    "retail_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da89d98-a7c3-4f2b-bbc0-7fada4602128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Total Invoice Amount Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90932b66-b4ba-4eb2-86eb-788c202edf66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|Invoice|count|\n+-------+-----+\n| 490298|   40|\n| 491055|    9|\n| 491969|  548|\n| 494345|   21|\n| 495102|   83|\n|C495103|    1|\n| 497027|   53|\n| 500571|    1|\n| 502228|   34|\n| 509340|    6|\n| 509348|   16|\n| 510990|    1|\n| 514573|    4|\n| 519910|   56|\n| 520518|    3|\n| 524016|    3|\n| 526812|    1|\n| 527367|    8|\n| 529286|    1|\n| 532542|    6|\n+-------+-----+\nonly showing top 20 rows\nDataframe statistics\nMin: 1\nMax: 1350\nMedian: 9.0\nMode: 1\nMean: 19.90324084433505\nDataframe statistics filtered by removing high outliers (85th percentile and above)\nMin: 1\nMax: 33\nMedian: 6.0\nMode: 1\nMean: 9.406085766020404\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_invoice_df = retail_df.groupBy(\"Invoice\").count()\n",
    "total_invoice_df.show()\n",
    "\n",
    "print(\"Dataframe statistics\")\n",
    "print(\"Min: \" + str(total_invoice_df.agg({'count': 'min'}).collect()[0][0]))\n",
    "print(\"Max: \" + str(total_invoice_df.agg({'count': 'max'}).collect()[0][0]))\n",
    "print(\"Median: \" + str(total_invoice_df.agg({'count': 'median'}).collect()[0][0]))\n",
    "print(\"Mode: \" + str(total_invoice_df.agg({'count': 'mode'}).collect()[0][0]))\n",
    "print(\"Mean: \" + str(total_invoice_df.agg({'count': 'mean'}).collect()[0][0]))\n",
    "\n",
    "eighty_fifth_quantile_val = total_invoice_df.approxQuantile(\"count\", [0.85], 0.01)\n",
    "quantile_filtered_total_invoice_df = total_invoice_df.filter((col(\"count\") <= eighty_fifth_quantile_val[0]))\n",
    "\n",
    "print(\"Dataframe statistics filtered by removing high outliers (85th percentile and above)\")\n",
    "print(\"Min: \" + str(quantile_filtered_total_invoice_df.agg({'count': 'min'}).collect()[0][0]))\n",
    "print(\"Max: \" + str(quantile_filtered_total_invoice_df.agg({'count': 'max'}).collect()[0][0]))\n",
    "print(\"Median: \" + str(quantile_filtered_total_invoice_df.agg({'count': 'median'}).collect()[0][0]))\n",
    "print(\"Mode: \" + str(quantile_filtered_total_invoice_df.agg({'count': 'mode'}).collect()[0][0]))\n",
    "print(\"Mean: \" + str(quantile_filtered_total_invoice_df.agg({'count': 'mean'}).collect()[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6530bc-3100-4320-be5a-2b3eb89d5be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monthly Placed and Canceled Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6849d5-483c-4930-8155-cf5a9db13f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+------------+\n|yyyymm|placed|cancelled|total_placed|\n+------+------+---------+------------+\n|201004| 33431|      626|       32179|\n|201002| 28812|      576|       27660|\n|201008| 32733|      573|       31587|\n|201105| 36409|      621|       35167|\n|201001| 30869|      686|       29497|\n|200912| 44213|     1015|       42183|\n|201106| 36163|      711|       34741|\n|201010| 58057|     1041|       55975|\n|201003| 40667|      844|       38979|\n|201103| 36049|      699|       34651|\n|201011| 76821|     1194|       74433|\n|201101| 34446|      701|       33044|\n|201006| 39190|      793|       37604|\n|201104| 29357|      559|       28239|\n|201005| 34340|      983|       32374|\n|201102| 27232|      475|       26282|\n|201012| 63947|     1057|       61833|\n|201007| 32649|      734|       31181|\n|201009| 41279|      812|       39655|\n|201107| 38833|      685|       37463|\n+------+------+---------+------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "yyyymm_df = retail_df.withColumn(\"yyyymm\", date_format(col(\"InvoiceDate\"), \"yyyyMM\"))\n",
    "\n",
    "monthly_cancelled_df = yyyymm_df.filter(col(\"invoice\").contains(\"C\")) \\\n",
    "    .groupBy(col(\"yyyymm\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"yyyymm\") \\\n",
    "    .withColumnRenamed(\"count\", \"cancelled\")\n",
    "\n",
    "monthly_placed_df = yyyymm_df.filter(~col(\"invoice\").contains(\"C\")) \\\n",
    "    .groupBy(col(\"yyyymm\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"yyyymm\") \\\n",
    "    .withColumnRenamed(\"count\", \"placed\")\n",
    "\n",
    "monthly_placed_cancelled_df = monthly_placed_df.join(monthly_cancelled_df, \"yyyymm\", \"left\") \\\n",
    "    .withColumn(\"total_placed\", col(\"placed\") - 2*col(\"cancelled\"))\n",
    "\n",
    "monthly_placed_cancelled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61c79fb-3952-4e3d-a033-ee0c81b870f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monthly Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ff2f91a-a788-4321-b724-60586d191f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n|yyyymm|             sales|\n+------+------------------+\n|200912| 799847.1100000143|\n|201001| 624032.8919999956|\n|201002| 533091.4260000042|\n|201003| 765848.7609999765|\n|201004| 590580.4319999823|\n|201005| 615322.8300000005|\n|201006| 679786.6099999842|\n|201007|  575236.359999999|\n|201008| 656776.3399999854|\n|201009| 853650.4309999745|\n|201010|1045168.3499998983|\n|201011|1422654.6419998251|\n|201012|1126445.4699999166|\n|201101| 560000.2600000234|\n|201102| 498062.6500000268|\n|201103| 683267.0800000189|\n|201104| 493207.1210000249|\n|201105|   723333.51000001|\n|201106| 691123.1200000157|\n|201107| 681300.1110000301|\n+------+------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "yyyymm_df = yyyymm_df.withColumn(\"total_sales\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "monthly_sales_df = yyyymm_df.groupBy(\"yyyymm\").agg({\"total_sales\": \"sum\"}).orderBy(\"yyyymm\").select(col(\"yyyymm\"), col(\"sum(total_sales)\").alias(\"sales\"))\n",
    "\n",
    "monthly_sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4f56a4-5343-4b02-a8e7-28589ca94a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monthly Sales Growth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1d38b7-2aa8-4e25-b823-1269bc573668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------------------+\n|yyyymm|             sales|percent_sales_growth|\n+------+------------------+--------------------+\n|200912| 799847.1100000143|                NULL|\n|201001| 624032.8919999956| -28.173870360670023|\n|201002| 533091.4260000042| -17.059262551334054|\n|201003| 765848.7609999765|   30.39207567510572|\n|201004| 590580.4319999823| -29.677300415534162|\n|201005| 615322.8300000005|   4.021043392785895|\n|201006| 679786.6099999842|   9.482943478393185|\n|201007|  575236.359999999|  -18.17518106817609|\n|201008| 656776.3399999854|  12.415182313051682|\n|201009| 853650.4309999745|  23.062612499283674|\n|201010|1045168.3499998983|    18.3241215637598|\n|201011|1422654.6419998251|  26.533937391108108|\n|201012|1126445.4699999166| -26.295917546717146|\n|201101| 560000.2600000234| -101.15088339420942|\n|201102| 498062.6500000268| -12.435706632487555|\n|201103| 683267.0800000189|  27.105715381456253|\n|201104| 493207.1210000249| -38.535526132434825|\n|201105|   723333.51000001|  31.814700386268836|\n|201106| 691123.1200000157|  -4.660586380035091|\n|201107| 681300.1110000301| -1.4418035226160721|\n+------+------------------+--------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, lit\n",
    "\n",
    "window = Window.orderBy(\"yyyymm\").partitionBy(lit(0))\n",
    "\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"percent_sales_growth\",\n",
    "                                               ((col(\"sales\") - lag(\"sales\").over(window))/(col(\"sales\")))*100)\n",
    "monthly_sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad613ef0-8605-40af-bfdd-bf0980fdb2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monthly Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ff6c2d-ab47-455a-8c99-f99e1f88c4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n|yyyymm|count(1)|\n+------+--------+\n|200912|    1046|\n|201001|     787|\n|201002|     808|\n|201003|    1112|\n|201004|     999|\n|201005|    1063|\n|201006|    1096|\n|201007|     989|\n|201008|     965|\n|201009|    1203|\n|201010|    1578|\n|201011|    1684|\n|201012|     949|\n|201101|     784|\n|201102|     799|\n|201103|    1021|\n|201104|     900|\n|201105|    1080|\n|201106|    1052|\n|201107|     994|\n+------+--------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "unique_customer_df = yyyymm_df.dropDuplicates([\"yyyymm\", \"Customer ID\"])\n",
    "\n",
    "monthly_active_df = unique_customer_df.groupBy(\"yyyymm\").agg({\"*\": \"count\"}).orderBy(\"yyyymm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d690e00-f5e1-486e-81c6-17e1096144af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# New and Existing Users\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca54f393-ed2c-41ad-9179-0f63f68ebc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n|YYYYMM|new_users|ex_users|\n+------+---------+--------+\n|200912|     1045|    NULL|\n|201001|      394|     392|\n|201002|      363|     444|\n|201003|      436|     675|\n|201004|      291|     707|\n|201005|      254|     808|\n|201006|      269|     826|\n|201007|      183|     805|\n|201008|      158|     806|\n|201009|      242|     960|\n|201010|      379|    1198|\n|201011|      322|    1361|\n|201012|       77|     871|\n|201101|       71|     712|\n|201102|      123|     675|\n|201103|      178|     842|\n|201104|      105|     794|\n|201105|      108|     971|\n|201106|      108|     943|\n|201107|      102|     891|\n+------+---------+--------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "first_purchase_month_df = yyyymm_df.groupBy(\"Customer ID\").agg({\"YYYYMM\": \"min\"}).withColumnRenamed(\"min(YYYYMM)\", \"first_purchase_month\").orderBy(\"Customer ID\")\n",
    "\n",
    "first_purchase_month_df = yyyymm_df.join(first_purchase_month_df, \"Customer ID\", \"inner\")\n",
    "\n",
    "new_users_df = first_purchase_month_df.filter(col(\"YYYYMM\") == col(\"first_purchase_month\")).dropDuplicates([\"Customer ID\", \"YYYYMM\"]).groupBy(\"YYYYMM\").agg({\"*\": \"count\"}).withColumnRenamed(\"count(1)\", \"new_users\").orderBy(\"YYYYMM\")\n",
    "ex_users_df = first_purchase_month_df.filter(col(\"YYYYMM\") != col(\"first_purchase_month\")).dropDuplicates([\"Customer ID\", \"YYYYMM\"]).groupBy(\"YYYYMM\").agg({\"*\": \"count\"}).withColumnRenamed(\"count(1)\", \"ex_users\").orderBy(\"YYYYMM\")\n",
    "\n",
    "new_ex_users_df = new_users_df.join(ex_users_df, \"YYYYMM\", \"left\").orderBy(\"YYYYMM\")\n",
    "new_ex_users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8acca73f-2133-45bf-a188-7d22fe602310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Finding RFM\n",
    "\n",
    "RFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in the retail and professional services industries. ([wikipedia](https://en.wikipedia.org/wiki/RFM_(market_research)))\n",
    "\n",
    "RFM stands for three dimensions:\n",
    "\n",
    "- Recency – How recently did the customer purchase?\n",
    "\n",
    "- Frequency – How often do they purchase?\n",
    "\n",
    "- Monetary Value – How much do they spend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2af5e6-1bd3-4132-8656-9ab9f6b6fc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------------+\n|Customer ID|       total_sales|days_since_last_purchase|\n+-----------+------------------+------------------------+\n|    18071.0|1428.6299999999999|                     257|\n|    13641.0| 921.8200000000002|                     640|\n|    16575.0|            -10.49|                     682|\n|    15512.0|331.58000000000004|                     155|\n|    17838.0|6239.5499999999965|                      26|\n|    12782.0| 4526.039999999998|                       4|\n|    12408.0| 4279.639999999999|                      32|\n|    14364.0|3717.3500000000004|                     108|\n|    13112.0|             -5.44|                     541|\n|    17328.0|             104.7|                     673|\n|    16547.0|111.71000000000001|                     546|\n|    13617.0|1263.8000000000002|                      40|\n|    13172.0|              69.6|                     400|\n|    16471.0|1186.1800000000007|                     274|\n|    14835.0|             293.1|                     527|\n|    17470.0| 2804.869999999999|                      44|\n|    17094.0|             370.4|                     318|\n|    13303.0|             61.72|                     400|\n|    18076.0|            151.49|                     380|\n|    14162.0|            289.36|                     173|\n+-----------+------------------+------------------------+\nonly showing top 20 rows\n+-----------+-------+------------------+\n|Customer ID|Invoice|  sum(total_sales)|\n+-----------+-------+------------------+\n|       NULL| 491969| 6141.939999999991|\n|    17841.0| 492784| 403.5599999999998|\n|    15044.0| 494046|              47.4|\n|    15719.0|C495930|             -5.95|\n|    13584.0| 496493|               1.3|\n|       NULL| 500571|              3.75|\n|    13179.0| 503295| 572.3100000000001|\n|    17867.0| 505152|             43.75|\n|    18135.0| 506222|             97.25|\n|    12725.0| 508637|             220.0|\n|    16717.0| 509028|            393.49|\n|    14047.0|C509297|             -4.25|\n|    14156.0| 510775|            1130.4|\n|    16133.0| 515023|            300.65|\n|    14922.0| 516343| 540.2900000000001|\n|    17732.0| 521820|            209.24|\n|    14646.0| 524969|2971.0600000000004|\n|    14911.0| 525013|1280.1600000000005|\n|    15958.0| 527457|131.10000000000002|\n|    17829.0| 529095| 76.32000000000001|\n+-----------+-------+------------------+\nonly showing top 20 rows\n+-----------+-----------------------+\n|Customer ID|count(sum(total_sales))|\n+-----------+-----------------------+\n|    12408.0|                     14|\n|    12782.0|                     14|\n|    17838.0|                     15|\n|    18071.0|                      6|\n|    13641.0|                      7|\n|    15512.0|                      4|\n|    14364.0|                      8|\n|    16575.0|                      1|\n|    13617.0|                      9|\n|    13172.0|                      1|\n|    13112.0|                      3|\n|    16547.0|                      1|\n|    17328.0|                      1|\n|    13160.0|                      1|\n|    13236.0|                      3|\n|    15643.0|                      6|\n|    17470.0|                     11|\n|    14835.0|                      3|\n|    18076.0|                      1|\n|    17094.0|                      3|\n+-----------+-----------------------+\nonly showing top 20 rows\n+-----------+------------------+-------+---------+\n|Customer ID|          Monetary|Recency|Frequency|\n+-----------+------------------+-------+---------+\n|    12408.0| 4279.639999999999|     32|       14|\n|    12782.0| 4526.039999999998|      4|       14|\n|    17838.0|6239.5499999999965|     26|       15|\n|    18071.0|1428.6299999999999|    257|        6|\n|    13641.0| 921.8200000000002|    640|        7|\n|    15512.0|331.58000000000004|    155|        4|\n|    14364.0|3717.3500000000004|    108|        8|\n|    16575.0|            -10.49|    682|        1|\n|    13617.0|1263.8000000000002|     40|        9|\n|    13172.0|              69.6|    400|        1|\n|    13112.0|             -5.44|    541|        3|\n|    16547.0|111.71000000000001|    546|        1|\n|    17328.0|             104.7|    673|        1|\n|    13160.0|              91.8|     22|        1|\n|    13236.0| 727.2700000000001|     24|        3|\n|    15643.0| 647.4000000000003|    179|        6|\n|    17470.0| 2804.869999999999|     44|       11|\n|    14835.0|             293.1|    527|        3|\n|    18076.0|            151.49|    380|        1|\n|    17094.0|             370.4|    318|        3|\n+-----------+------------------+-------+---------+\nonly showing top 20 rows\n+-----------+------------+--------------+-------------+\n|Customer ID|RecencyScore|FrequencyScore|MonetaryScore|\n+-----------+------------+--------------+-------------+\n|    17399.0|           1|             1|            1|\n|    12918.0|           1|             3|            1|\n|    15849.0|           1|             1|            1|\n|    15760.0|           1|             4|            1|\n|    16981.0|           1|             1|            1|\n|    16151.0|           1|             1|            1|\n|    14063.0|           1|             5|            1|\n|    18023.0|           1|             1|            1|\n|    17013.0|           1|             2|            1|\n|    15202.0|           1|             4|            1|\n|    15413.0|           1|             4|            1|\n|    15896.0|           1|             2|            1|\n|    13401.0|           1|             2|            1|\n|    13100.0|           1|             2|            1|\n|    14802.0|           1|             4|            1|\n|    14255.0|           1|             3|            1|\n|    14213.0|           2|             2|            1|\n|    13335.0|           1|             1|            1|\n|    14939.0|           1|             2|            1|\n|    14308.0|           1|             4|            1|\n+-----------+------------+--------------+-------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, datediff, lit, expr, ntile, col, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "date_end = retail_df.select(col(\"InvoiceDate\")).agg({\"InvoiceDate\": \"max\"}).collect()[0][0]\n",
    "\n",
    "df_x = yyyymm_df.groupBy(\"Customer ID\").agg(\n",
    "    expr(\"sum(total_sales) as total_sales\"),\n",
    "    expr(f\"datediff('{date_end}', max(InvoiceDate)) as days_since_last_purchase\")\n",
    ")\n",
    "df_x.show()\n",
    "\n",
    "df_y = yyyymm_df.groupBy(\"Customer ID\", \"Invoice\").agg(\n",
    "    {\"total_sales\": \"sum\"}\n",
    ")\n",
    "df_y.show()\n",
    "\n",
    "df_z = df_y.groupBy(\"Customer ID\").agg(\n",
    "    {\"sum(total_sales)\": \"count\"}\n",
    ")\n",
    "df_z.show()\n",
    "\n",
    "rfm_table = df_x.join(df_z, \"Customer ID\", \"inner\")\n",
    "rfm_table = rfm_table.withColumnsRenamed({\"days_since_last_purchase\": \"Recency\",\n",
    "                              \"count(sum(total_sales))\": \"Frequency\",\n",
    "                              \"total_sales\": \"Monetary\"})\n",
    "rfm_table.show()\n",
    "\n",
    "window_recency = Window.orderBy('Recency').partitionBy(lit(0))\n",
    "df_recency = rfm_table.withColumn(\n",
    "    'RecencyScore', \n",
    "    ntile(5).over(window_recency)\n",
    ")\n",
    "\n",
    "window_frequency = Window.orderBy('Frequency').partitionBy(lit(0))\n",
    "df_frequency = df_recency.withColumn(\n",
    "    'FrequencyScore',\n",
    "    ntile(5).over(window_frequency)\n",
    ")\n",
    "\n",
    "window_monetary = Window.orderBy('Monetary').partitionBy(lit(0))\n",
    "df_monetary = df_frequency.withColumn(\n",
    "    'MonetaryScore',\n",
    "    ntile(5).over(window_monetary)\n",
    ")\n",
    "\n",
    "df_rfm = df_monetary.withColumn(\n",
    "    'RecencyScore', \n",
    "    6 - col('RecencyScore')\n",
    ")\n",
    "\n",
    "df_rfm.select(\"Customer ID\", \"RecencyScore\", \"FrequencyScore\", \"MonetaryScore\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f3d340-2a65-4aac-837c-eb253ab6943b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RFM Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c33d03-9132-4143-99dc-f739cbc8dc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+--------------+\n|            Segment|      Mean_Recency|    Mean_Frequency|     Mean_Monetary|Customer_Count|\n+-------------------+------------------+------------------+------------------+--------------+\n|        Hibernating|1.4085526315789474| 1.499342105263158|1.6736842105263159|          1520|\n|            At Risk|1.6443569553805775|3.4173228346456694|  3.02755905511811|           762|\n|         Can't Lose|1.8085106382978724|               5.0| 4.361702127659575|            94|\n|     Need Attention|               3.0|               3.0| 2.937984496124031|           258|\n|    Loyal Customers|3.5281569965870307| 4.454778156996587| 4.238907849829351|          1172|\n|Potential Loyalists| 4.441798941798941| 2.642857142857143|2.8214285714285716|           756|\n|      New Customers|               5.0|               1.0| 1.619047619047619|            63|\n|     About to Sleep|               3.0|1.4244031830238726|1.9283819628647214|           377|\n|          Promising|               4.0|               1.0|1.5608108108108107|           148|\n|          Champions|               5.0| 4.708333333333333| 4.599747474747475|           792|\n+-------------------+------------------+------------------+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, concat, mean, count\n",
    "\n",
    "df_rfm = df_rfm.withColumn(\n",
    "    'Segment', \n",
    "    concat(col('RecencyScore').cast('string'), col('FrequencyScore').cast('string'))\n",
    ")\n",
    "\n",
    "df_rfm = df_rfm.withColumn(\n",
    "    'Segment', \n",
    "    when(col('Segment').rlike(r'[1-2][1-2]'), 'Hibernating')\n",
    "    .when(col('Segment').rlike(r'[1-2][3-4]'), 'At Risk')\n",
    "    .when(col('Segment').rlike(r'[1-2]5'), 'Can\\'t Lose')\n",
    "    .when(col('Segment').rlike(r'3[1-2]'), 'About to Sleep')\n",
    "    .when(col('Segment').rlike(r'33'), 'Need Attention')\n",
    "    .when(col('Segment').rlike(r'[3-4][4-5]'), 'Loyal Customers')\n",
    "    .when(col('Segment').rlike(r'41'), 'Promising')\n",
    "    .when(col('Segment').rlike(r'51'), 'New Customers')\n",
    "    .when(col('Segment').rlike(r'[4-5][2-3]'), 'Potential Loyalists')\n",
    "    .when(col('Segment').rlike(r'5[4-5]'), 'Champions')\n",
    "    .otherwise('Unknown')\n",
    ")\n",
    "\n",
    "agg_df = df_rfm.groupBy(\"Segment\").agg(\n",
    "    mean(\"RecencyScore\").alias(\"Mean_Recency\"),\n",
    "    mean(\"FrequencyScore\").alias(\"Mean_Frequency\"),\n",
    "    mean(\"MonetaryScore\").alias(\"Mean_Monetary\"),\n",
    "    count(\"Customer ID\").alias(\"Customer_Count\")\n",
    ")\n",
    "\n",
    "agg_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "retail_data_wrangling_template",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}